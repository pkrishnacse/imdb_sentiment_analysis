{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pos_train_data = pd.read_csv('train_pos.tsv',sep = '\\t')\n",
    "neg_train_data = pd.read_csv('train_neg.tsv',sep = '\\t')\n",
    "pos_test_data = pd.read_csv('test_pos.tsv',sep = '\\t')\n",
    "neg_test_data = pd.read_csv('test_neg.tsv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_train_data = pos_train_data[['Text','Sentiment']]\n",
    "neg_train_data = neg_train_data[['Text','Sentiment']]\n",
    "pos_test_data = pos_test_data[['Text','Sentiment']]\n",
    "neg_test_data = neg_test_data[['Text','Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Im still in doubt if this is just a horrible m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I used to watch this on either HBO or Showtime...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just watched this for the first time in a lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I found the writing in this movie absolutely t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In all honesty, this series is as much a class...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Im still in doubt if this is just a horrible m...          0\n",
       "1  I used to watch this on either HBO or Showtime...          1\n",
       "2  I just watched this for the first time in a lo...          1\n",
       "3  I found the writing in this movie absolutely t...          0\n",
       "4  In all honesty, this series is as much a class...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([pos_train_data,neg_train_data],ignore_index = True)\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie totally sucked!!! Don't even rent i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POSSIBLE SPOILERS --&lt;br /&gt;&lt;br /&gt;I love Dennis ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILERS*** ***SPOILERS*** What's going on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No one can argue with it. This IS and WILL BE ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fine young cast is wasted in this empty, maw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  This movie totally sucked!!! Don't even rent i...          0\n",
       "1  POSSIBLE SPOILERS --<br /><br />I love Dennis ...          0\n",
       "2  ***SPOILERS*** ***SPOILERS*** What's going on ...          0\n",
       "3  No one can argue with it. This IS and WILL BE ...          1\n",
       "4  A fine young cast is wasted in this empty, maw...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.concat([pos_test_data,neg_test_data],ignore_index = True)\n",
    "data_test = data_test.sample(frac=1).reset_index(drop=True)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', punctuation)\n",
    "\n",
    "def textclean(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im',\n",
       " 'still',\n",
       " 'doubt',\n",
       " 'horrible',\n",
       " 'movie',\n",
       " 'worse',\n",
       " 'movie',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'actors',\n",
       " 'painful',\n",
       " 'impossible',\n",
       " 'get',\n",
       " 'br',\n",
       " 'br',\n",
       " 'waist',\n",
       " 'time',\n",
       " 'movie',\n",
       " 'submitting',\n",
       " 'comment',\n",
       " 'agreeing',\n",
       " 'terms',\n",
       " 'laid',\n",
       " 'copyright',\n",
       " 'statement',\n",
       " 'submission',\n",
       " 'must',\n",
       " 'original',\n",
       " 'work',\n",
       " 'comments',\n",
       " 'normally',\n",
       " 'posted',\n",
       " 'site',\n",
       " 'within',\n",
       " 'business',\n",
       " 'days',\n",
       " 'comments',\n",
       " 'meet',\n",
       " 'guidelines',\n",
       " 'posted',\n",
       " 'please',\n",
       " 'write',\n",
       " 'english',\n",
       " 'html',\n",
       " 'boards',\n",
       " 'supported',\n",
       " 'though',\n",
       " 'paragraph',\n",
       " 'breaks',\n",
       " 'inserted',\n",
       " 'leave',\n",
       " 'blank',\n",
       " 'line',\n",
       " 'submitting',\n",
       " 'comment',\n",
       " 'agreeing',\n",
       " 'terms',\n",
       " 'laid',\n",
       " 'copyright',\n",
       " 'statement',\n",
       " 'submission',\n",
       " 'must',\n",
       " 'original',\n",
       " 'work',\n",
       " 'comments',\n",
       " 'normally',\n",
       " 'posted',\n",
       " 'site',\n",
       " 'within',\n",
       " 'business',\n",
       " 'days',\n",
       " 'comments',\n",
       " 'meet',\n",
       " 'guidelines',\n",
       " 'posted',\n",
       " 'please',\n",
       " 'write',\n",
       " 'english',\n",
       " 'html',\n",
       " 'boards',\n",
       " 'supported',\n",
       " 'though',\n",
       " 'paragraph',\n",
       " 'breaks',\n",
       " 'inserted',\n",
       " 'leave',\n",
       " 'blank',\n",
       " 'line',\n",
       " 'paragraph']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = []\n",
    "\n",
    "for index,row in data_train.iterrows():\n",
    "    text = (row['Text'].lower())    \n",
    "    reviews.append(textclean(text))\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'still'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "linked_reviews = list(itertools.chain.from_iterable(reviews))\n",
    "\n",
    "vocab_freq = dict()\n",
    "\n",
    "linked_reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in linked_reviews:\n",
    "    if word not in vocab_freq:\n",
    "        vocab_freq[word] = 1\n",
    "    else:\n",
    "        vocab_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capture': 281,\n",
       " 'mathematicians': 2,\n",
       " 'vidpic': 1,\n",
       " 'charactures': 1,\n",
       " 'cinematographers': 8,\n",
       " 'mystics': 5,\n",
       " 'blankman': 3,\n",
       " 'jeayes': 1,\n",
       " 'spiralled': 1,\n",
       " 'splattered': 12,\n",
       " 'stronghold': 3,\n",
       " 'katch': 2,\n",
       " 'proud': 181,\n",
       " 'diwana': 1,\n",
       " 'oldman': 7,\n",
       " 'spicing': 2,\n",
       " 'radioland': 1,\n",
       " 'kraft': 6,\n",
       " 'retromedia': 6,\n",
       " 'dawnfall': 1,\n",
       " 'baltic': 4,\n",
       " 'analogies': 9,\n",
       " 'seeming': 56,\n",
       " 'overplaying': 4,\n",
       " 'scorer': 1,\n",
       " 'kastle': 1,\n",
       " 'hulbert': 1,\n",
       " 'casual': 68,\n",
       " 'sheilah': 3,\n",
       " 'futuristic': 112,\n",
       " 'hillsides': 1,\n",
       " 'massacrenot': 1,\n",
       " 'inanely': 1,\n",
       " 'anecdotes': 10,\n",
       " 'socal': 3,\n",
       " 'experiments': 85,\n",
       " 'uselessly': 6,\n",
       " 'gremlin': 4,\n",
       " 'graff': 1,\n",
       " 'buster': 74,\n",
       " 'guility': 1,\n",
       " 'intricate': 57,\n",
       " 'domestication': 5,\n",
       " 'analyzer': 1,\n",
       " 'gameboys': 1,\n",
       " 'biryani': 2,\n",
       " 'feasible': 5,\n",
       " 'tampers': 1,\n",
       " 'discipleship': 1,\n",
       " 'mileage': 6,\n",
       " 'transporting': 10,\n",
       " 'aint': 6,\n",
       " 'cellophane': 1,\n",
       " 'scheffer': 2,\n",
       " 'dwarfs': 23,\n",
       " 'greenaway': 37,\n",
       " 'episodes': 919,\n",
       " 'tatou': 3,\n",
       " 'estes': 9,\n",
       " 'veterinarian': 5,\n",
       " 'gawping': 1,\n",
       " 'corey': 51,\n",
       " 'eventully': 1,\n",
       " 'nakajima': 1,\n",
       " 'southerland': 2,\n",
       " 'deflect': 4,\n",
       " 'nationalism': 6,\n",
       " 'sidious': 1,\n",
       " 'retailers': 1,\n",
       " 'antibiotics': 1,\n",
       " 'shutterbug': 1,\n",
       " 'kaedin': 1,\n",
       " 'grot': 8,\n",
       " 'frantic': 49,\n",
       " 'fujiko': 6,\n",
       " 'booze': 31,\n",
       " 'vistas': 18,\n",
       " 'ryunosuke': 3,\n",
       " 'transpose': 1,\n",
       " 'daydream': 6,\n",
       " 'raleigh': 2,\n",
       " 'probobly': 2,\n",
       " 'goodspeed': 1,\n",
       " 'snaking': 1,\n",
       " 'gentlemens': 1,\n",
       " 'darkling': 7,\n",
       " 'hijacked': 15,\n",
       " 'stathom': 1,\n",
       " 'houseguest': 1,\n",
       " 'impervious': 6,\n",
       " 'mukshin': 1,\n",
       " 'experiental': 1,\n",
       " 'oath': 9,\n",
       " 'initially': 173,\n",
       " 'bowie': 23,\n",
       " 'feinstein': 1,\n",
       " 'billing': 38,\n",
       " 'driftwood': 1,\n",
       " 'incarnations': 16,\n",
       " 'flue': 2,\n",
       " 'congas': 1,\n",
       " 'endulge': 1,\n",
       " 'yup': 26,\n",
       " 'interpretation': 160,\n",
       " 'pickle': 3,\n",
       " 'elves': 18,\n",
       " 'goyokin': 11,\n",
       " 'complete': 1021,\n",
       " 'contraception': 2,\n",
       " 'duperrey': 1,\n",
       " 'paternal': 7,\n",
       " 'klien': 2,\n",
       " 'ubertallented': 1,\n",
       " 'cubitt': 9,\n",
       " 'palermo': 25,\n",
       " 'subtle': 424,\n",
       " 'coward': 46,\n",
       " 'weekends': 8,\n",
       " 'juvenille': 2,\n",
       " 'dukakas': 1,\n",
       " 'dildo': 7,\n",
       " 'realists': 2,\n",
       " 'hybrid': 52,\n",
       " 'riscorla': 1,\n",
       " 'herpes': 2,\n",
       " 'cromoscope': 2,\n",
       " 'hacienda': 1,\n",
       " 'wearisome': 2,\n",
       " 'hogue': 1,\n",
       " 'imbecilic': 11,\n",
       " 'biker': 48,\n",
       " 'soundtract': 1,\n",
       " 'replicating': 3,\n",
       " 'expierence': 1,\n",
       " 'chives': 1,\n",
       " 'murders': 355,\n",
       " 'stature': 34,\n",
       " 'applicants': 2,\n",
       " 'skala': 3,\n",
       " 'shalub': 2,\n",
       " 'develops': 143,\n",
       " 'sissy': 82,\n",
       " 'acedmy': 1,\n",
       " 'braincell': 1,\n",
       " 'slavishly': 2,\n",
       " 'rock': 812,\n",
       " 'spawns': 6,\n",
       " 'whay': 1,\n",
       " 'jostles': 1,\n",
       " 'abductor': 4,\n",
       " 'shiris': 2,\n",
       " 'peculiar': 55,\n",
       " 'suffocated': 3,\n",
       " 'minstrel': 12,\n",
       " 'despots': 1,\n",
       " 'admire': 123,\n",
       " 'jfk': 20,\n",
       " 'blubbered': 1,\n",
       " 'rebooted': 1,\n",
       " 'length': 255,\n",
       " 'tonelessly': 1,\n",
       " 'coulouris': 16,\n",
       " 'wittle': 1,\n",
       " 'mest': 1,\n",
       " 'outlaws': 23,\n",
       " 'valga': 1,\n",
       " 'pânico': 1,\n",
       " 'pronto': 3,\n",
       " 'unreported': 3,\n",
       " 'stampede': 8,\n",
       " 'buff': 95,\n",
       " 'arahan': 2,\n",
       " 'apolitical': 2,\n",
       " 'kostic': 1,\n",
       " 'irishman': 11,\n",
       " 'proctor': 1,\n",
       " 'pundit': 1,\n",
       " 'player': 276,\n",
       " 'neutral': 29,\n",
       " 'uff': 1,\n",
       " 'keren': 1,\n",
       " 'adapt': 43,\n",
       " 'upham': 1,\n",
       " 'cloke': 1,\n",
       " 'hinako': 1,\n",
       " 'ebola': 2,\n",
       " 'additions': 24,\n",
       " 'eradicator': 1,\n",
       " 'mcdowell': 37,\n",
       " 'evicted': 9,\n",
       " 'bard': 14,\n",
       " 'proverb': 5,\n",
       " 'couture': 3,\n",
       " 'schone': 1,\n",
       " 'visualising': 1,\n",
       " 'santosh': 1,\n",
       " 'lira': 2,\n",
       " 'instructor': 28,\n",
       " 'messed': 84,\n",
       " 'traped': 1,\n",
       " 'dwelling': 18,\n",
       " 'gek': 2,\n",
       " 'siddons': 2,\n",
       " 'fretwell': 2,\n",
       " 'mohr': 18,\n",
       " 'drudge': 4,\n",
       " 'harnois': 1,\n",
       " 'appetite': 22,\n",
       " 'goers': 33,\n",
       " 'eeeeeeeek': 1,\n",
       " 'beng': 1,\n",
       " 'ancillary': 3,\n",
       " 'rendered': 64,\n",
       " 'tamo': 2,\n",
       " 'kramp': 4,\n",
       " 'pras': 1,\n",
       " 'porshe': 1,\n",
       " 'foppishly': 1,\n",
       " 'puerility': 1,\n",
       " 'lorenço': 1,\n",
       " 'annihilated': 6,\n",
       " 'convention': 50,\n",
       " 'bye': 53,\n",
       " 'reve': 21,\n",
       " 'unfotunately': 1,\n",
       " 'wender': 1,\n",
       " 'chafes': 1,\n",
       " 'pelham': 2,\n",
       " 'byproduct': 6,\n",
       " 'vivacious': 12,\n",
       " 'apologists': 4,\n",
       " 'wail': 1,\n",
       " 'rapp': 8,\n",
       " 'contemperaneous': 1,\n",
       " 'rudimentary': 18,\n",
       " 'unflattering': 11,\n",
       " 'behaviour': 75,\n",
       " 'gluttonous': 3,\n",
       " 'acting': 6335,\n",
       " 'solondz': 1,\n",
       " 'kagaz': 1,\n",
       " 'invulnerability': 4,\n",
       " 'percentages': 3,\n",
       " 'trey': 26,\n",
       " 'anchía': 1,\n",
       " 'waching': 1,\n",
       " 'bookman': 2,\n",
       " 'wearier': 1,\n",
       " 'hedeen': 1,\n",
       " 'pooch': 9,\n",
       " 'giusstissia': 1,\n",
       " 'shuttlecrafts': 1,\n",
       " 'kacia': 1,\n",
       " 'allusions': 28,\n",
       " 'repo': 9,\n",
       " 'townie': 1,\n",
       " 'buerke': 1,\n",
       " 'arrangements': 23,\n",
       " 'roofthooft': 1,\n",
       " 'phenomena': 19,\n",
       " 'glints': 1,\n",
       " 'mfn': 2,\n",
       " 'decaune': 1,\n",
       " 'staged': 101,\n",
       " 'quartermain': 3,\n",
       " 'majorcan': 1,\n",
       " 'motorcars': 3,\n",
       " 'interest': 986,\n",
       " 'looting': 10,\n",
       " 'saucers': 5,\n",
       " 'allégret': 2,\n",
       " 'brandauer': 4,\n",
       " 'predicament': 28,\n",
       " 'lyu': 4,\n",
       " 'unrecycled': 1,\n",
       " 'trios': 2,\n",
       " 'barring': 9,\n",
       " 'gunfighter': 13,\n",
       " 'ortelli': 1,\n",
       " 'fallon': 51,\n",
       " 'synchronization': 6,\n",
       " 'possessiveness': 1,\n",
       " 'aquafresh': 3,\n",
       " 'viktor': 4,\n",
       " 'frenzies': 1,\n",
       " 'documenter': 1,\n",
       " 'amping': 4,\n",
       " 'flirty': 3,\n",
       " 'unhesitatingly': 1,\n",
       " 'vicey': 1,\n",
       " 'gordious': 1,\n",
       " 'staunch': 11,\n",
       " 'perseverence': 1,\n",
       " 'divide': 16,\n",
       " 'kamal': 33,\n",
       " 'fabrizio': 4,\n",
       " 'rehumanization': 1,\n",
       " 'deuce': 6,\n",
       " 'hounsou': 1,\n",
       " 'gangbangers': 3,\n",
       " 'lucinda': 5,\n",
       " 'sl': 1,\n",
       " 'blown': 166,\n",
       " 'soaks': 1,\n",
       " 'backer': 1,\n",
       " 'hoodwinks': 1,\n",
       " 'intently': 8,\n",
       " 'pryor': 27,\n",
       " 'slowest': 7,\n",
       " 'dorma': 3,\n",
       " 'grahame': 8,\n",
       " 'unstoned': 1,\n",
       " 'clarkson': 18,\n",
       " 'beachcombers': 1,\n",
       " 'dignify': 3,\n",
       " 'flatop': 1,\n",
       " 'incomprehensibly': 4,\n",
       " 'guidelines': 17,\n",
       " 'werewolf': 261,\n",
       " 'millennial': 2,\n",
       " 'santini': 2,\n",
       " 'johnathon': 4,\n",
       " 'misspelled': 1,\n",
       " 'quantrill': 6,\n",
       " 'cocky': 34,\n",
       " 'compared': 534,\n",
       " 'tibetans': 2,\n",
       " 'dishes': 18,\n",
       " 'zoom': 28,\n",
       " 'buzzes': 1,\n",
       " 'elucidated': 2,\n",
       " 'hatcher': 19,\n",
       " 'frequented': 4,\n",
       " 'thundiiayil': 1,\n",
       " 'trendsetter': 1,\n",
       " 'dairy': 4,\n",
       " 'krocodylus': 2,\n",
       " 'entereth': 1,\n",
       " 'skin': 195,\n",
       " 'pippi': 5,\n",
       " 'baddiel': 2,\n",
       " 'goes': 2427,\n",
       " 'milne': 2,\n",
       " 'wiggins': 1,\n",
       " 'kiosk': 1,\n",
       " 'gerolmo': 4,\n",
       " 'revivals': 1,\n",
       " 'denouement': 47,\n",
       " 'desando': 1,\n",
       " 'mater': 8,\n",
       " 'delon': 25,\n",
       " 'abuelita': 1,\n",
       " 'hrshitta': 1,\n",
       " 'tarnish': 6,\n",
       " 'fruits': 8,\n",
       " 'kastner': 2,\n",
       " 'coughing': 2,\n",
       " 'zelenka': 2,\n",
       " 'sweltering': 6,\n",
       " 'schoolboys': 3,\n",
       " 'erupts': 17,\n",
       " 'restriction': 6,\n",
       " 'winslett': 3,\n",
       " 'reviewing': 56,\n",
       " 'superkicked': 1,\n",
       " 'quatermass': 6,\n",
       " 'mononoke': 18,\n",
       " 'messing': 57,\n",
       " 'inland': 8,\n",
       " 'communications': 15,\n",
       " 'ozymandias': 1,\n",
       " 'absorbent': 1,\n",
       " 'unmodernized': 1,\n",
       " 'disappearance': 41,\n",
       " 'multicultural': 2,\n",
       " 'ejaculate': 4,\n",
       " 'johnsons': 6,\n",
       " 'identified': 39,\n",
       " 'tlps': 3,\n",
       " 'dignity': 110,\n",
       " 'kongs': 1,\n",
       " 'appealing': 223,\n",
       " 'truckload': 6,\n",
       " 'congradulations': 1,\n",
       " 'pengium': 1,\n",
       " 'bladed': 2,\n",
       " 'galatica': 1,\n",
       " 'indianised': 1,\n",
       " 'pendelton': 5,\n",
       " 'kipling': 25,\n",
       " 'thang': 3,\n",
       " 'inauguration': 4,\n",
       " 'scratching': 39,\n",
       " 'ordering': 20,\n",
       " 'incarcerations': 1,\n",
       " 'arklie': 1,\n",
       " 'wrestle': 10,\n",
       " 'unbridled': 9,\n",
       " 'flintstones': 3,\n",
       " 'oughts': 2,\n",
       " 'capitulation': 1,\n",
       " 'dykes': 1,\n",
       " 'vulturine': 1,\n",
       " 'moviegoing': 2,\n",
       " 'snag': 10,\n",
       " 'zinemann': 1,\n",
       " 'cringe': 80,\n",
       " 'thatcherite': 1,\n",
       " 'ariauna': 6,\n",
       " 'emelius': 2,\n",
       " 'closed': 81,\n",
       " 'mountainside': 5,\n",
       " 'bloodlust': 6,\n",
       " 'hoaky': 1,\n",
       " 'idea': 2000,\n",
       " 'jasn': 1,\n",
       " 'positronic': 1,\n",
       " 'typist': 4,\n",
       " 'plain': 560,\n",
       " 'underage': 12,\n",
       " 'vander': 1,\n",
       " 'arzenta': 9,\n",
       " 'dumpty': 1,\n",
       " 'estimations': 1,\n",
       " 'particullary': 1,\n",
       " 'knox': 30,\n",
       " 'show': 6123,\n",
       " 'bart': 39,\n",
       " 'ninth': 15,\n",
       " 'mimicking': 14,\n",
       " 'devolved': 3,\n",
       " 'animatronic': 7,\n",
       " 'jarring': 59,\n",
       " 'jirarudan': 1,\n",
       " 'whistled': 1,\n",
       " 'counseling': 12,\n",
       " 'brochure': 4,\n",
       " 'devos': 25,\n",
       " 'sawed': 4,\n",
       " 'fastball': 1,\n",
       " 'benward': 7,\n",
       " 'parasomnia': 4,\n",
       " 'oscillating': 2,\n",
       " 'hariett': 1,\n",
       " 'soothe': 6,\n",
       " 'porn': 338,\n",
       " 'furnished': 8,\n",
       " 'costello': 26,\n",
       " 'dustbin': 7,\n",
       " 'boundary': 5,\n",
       " 'idolise': 1,\n",
       " 'offensiveness': 3,\n",
       " 'rotted': 6,\n",
       " 'splats': 1,\n",
       " 'darwininan': 1,\n",
       " 'aristos': 1,\n",
       " 'infancy': 10,\n",
       " 'choreographing': 4,\n",
       " 'proffering': 1,\n",
       " 'walberg': 1,\n",
       " 'prefabricated': 1,\n",
       " 'petra': 7,\n",
       " 'resolutely': 6,\n",
       " 'radio': 272,\n",
       " 'electrical': 20,\n",
       " 'lamas': 33,\n",
       " 'perceptively': 1,\n",
       " 'mentoring': 2,\n",
       " 'carbon': 23,\n",
       " 'reflections': 25,\n",
       " 'duffel': 5,\n",
       " 'wardrobe': 60,\n",
       " 'consistency': 27,\n",
       " 'undisputed': 16,\n",
       " 'meanly': 1,\n",
       " 'graaff': 6,\n",
       " 'hum': 41,\n",
       " 'distinguish': 28,\n",
       " 'haplessly': 1,\n",
       " 'timur': 1,\n",
       " 'functionally': 1,\n",
       " 'helfer': 1,\n",
       " 'creaked': 3,\n",
       " 'maturity': 36,\n",
       " 'frankenfish': 1,\n",
       " 'thanatopsis': 1,\n",
       " 'boobless': 1,\n",
       " 'unrevealed': 2,\n",
       " 'latham': 10,\n",
       " 'shets': 1,\n",
       " 'vassar': 1,\n",
       " 'yak': 4,\n",
       " 'federally': 1,\n",
       " 'mesopotamia': 3,\n",
       " 'headband': 1,\n",
       " 'seismic': 2,\n",
       " 'catalysis': 1,\n",
       " 'tendons': 1,\n",
       " 'betts': 2,\n",
       " 'robbers': 48,\n",
       " 'elisha': 35,\n",
       " 'memorable': 650,\n",
       " 'emeritus': 1,\n",
       " 'antiquities': 6,\n",
       " 'lachrymose': 4,\n",
       " 'véronika': 1,\n",
       " 'pulling': 119,\n",
       " 'overtake': 4,\n",
       " 'chappie': 2,\n",
       " 'pueblos': 1,\n",
       " 'concussive': 1,\n",
       " 'kar': 8,\n",
       " 'tatsuhito': 9,\n",
       " 'clearence': 1,\n",
       " 'matched': 75,\n",
       " 'palatir': 1,\n",
       " 'drool': 16,\n",
       " 'canners': 1,\n",
       " 'monarchs': 5,\n",
       " 'strasse': 1,\n",
       " 'jupiter': 6,\n",
       " 'supplements': 1,\n",
       " 'grizly': 1,\n",
       " 'appoints': 2,\n",
       " 'rollnecks': 1,\n",
       " 'nausium': 1,\n",
       " 'fatalism': 10,\n",
       " 'navel': 1,\n",
       " 'gorns': 1,\n",
       " 'dugal': 1,\n",
       " 'overemotional': 2,\n",
       " 'bores': 28,\n",
       " 'cuttrell': 1,\n",
       " 'puddles': 3,\n",
       " 'mário': 1,\n",
       " 'spritely': 1,\n",
       " 'maloni': 5,\n",
       " 'gaetani': 1,\n",
       " 'chill': 54,\n",
       " 'povich': 1,\n",
       " 'fi': 73,\n",
       " 'hillbilly': 24,\n",
       " 'ploughs': 1,\n",
       " 'florinda': 11,\n",
       " 'bomba': 1,\n",
       " 'garishly': 2,\n",
       " 'monolog': 1,\n",
       " 'plotty': 1,\n",
       " 'mellowed': 1,\n",
       " 'carapace': 1,\n",
       " 'childe': 1,\n",
       " 'kinnair': 1,\n",
       " 'magwood': 1,\n",
       " 'butcherer': 1,\n",
       " 'monte': 14,\n",
       " 'inadvertantly': 1,\n",
       " 'stellwaggen': 1,\n",
       " 'shooked': 1,\n",
       " 'nymphomaniacal': 1,\n",
       " 'leader': 238,\n",
       " 'lovestruck': 3,\n",
       " 'transmits': 4,\n",
       " 'fooling': 19,\n",
       " 'sympathizer': 8,\n",
       " 'biographers': 2,\n",
       " 'imperatives': 3,\n",
       " 'dwindles': 3,\n",
       " 'kinfolk': 2,\n",
       " 'witchfinder': 3,\n",
       " 'hardyz': 1,\n",
       " 'tuesday': 15,\n",
       " 'enacting': 3,\n",
       " 'antwortet': 1,\n",
       " 'gielguld': 1,\n",
       " 'lmao': 4,\n",
       " 'psychlogical': 1,\n",
       " 'cooler': 24,\n",
       " 'quickest': 5,\n",
       " 'hirsute': 5,\n",
       " 'bovasso': 1,\n",
       " 'whodunnits': 2,\n",
       " 'lackadaisical': 5,\n",
       " 'dermatonecrotic': 1,\n",
       " 'exc': 1,\n",
       " 'humorists': 4,\n",
       " 'late': 1121,\n",
       " 'ignacio': 1,\n",
       " 'herinteractive': 1,\n",
       " 'centerline': 1,\n",
       " 'cruelties': 3,\n",
       " 'charle': 1,\n",
       " 'coincide': 9,\n",
       " 'massive': 191,\n",
       " 'shallowest': 2,\n",
       " 'athletes': 16,\n",
       " 'dupery': 1,\n",
       " 'celina': 6,\n",
       " 'undifferentiated': 2,\n",
       " 'charleson': 1,\n",
       " 'heartbroken': 15,\n",
       " 'empties': 1,\n",
       " 'galumphing': 1,\n",
       " 'vibrant': 68,\n",
       " 'yeeshhhhhhhhhhhhhhhhh': 1,\n",
       " 'hutt': 7,\n",
       " 'fiascos': 1,\n",
       " 'barrage': 17,\n",
       " 'predispose': 1,\n",
       " 'asturias': 1,\n",
       " 'corigliano': 1,\n",
       " 'refugee': 26,\n",
       " 'detroit': 49,\n",
       " 'properly': 161,\n",
       " 'kazakhstani': 1,\n",
       " 'holdaway': 1,\n",
       " 'stefania': 5,\n",
       " 'brassiere': 1,\n",
       " 'call': 908,\n",
       " 'macgregor': 20,\n",
       " 'hindered': 13,\n",
       " 'yes': 1505,\n",
       " 'destructed': 2,\n",
       " 'haden': 8,\n",
       " 'ampas': 2,\n",
       " 'pioneer': 23,\n",
       " 'unstudied': 2,\n",
       " 'recommends': 6,\n",
       " 'pernell': 8,\n",
       " 'scouting': 3,\n",
       " 'napalm': 9,\n",
       " 'wm': 2,\n",
       " 'rebut': 1,\n",
       " 'thunderstorms': 1,\n",
       " 'consultant': 16,\n",
       " 'alludes': 7,\n",
       " 'kool': 3,\n",
       " 'supermutant': 1,\n",
       " 'doubtful': 21,\n",
       " 'sanitary': 2,\n",
       " 'conflictive': 1,\n",
       " 'ticker': 1,\n",
       " 'highbrow': 6,\n",
       " 'rearranged': 3,\n",
       " 'mamas': 2,\n",
       " 'merchandising': 7,\n",
       " 'niellson': 1,\n",
       " 'allures': 1,\n",
       " 'perestroika': 5,\n",
       " 'lemmings': 4,\n",
       " 'proft': 1,\n",
       " 'chartreuse': 2,\n",
       " 'bellocchio': 2,\n",
       " 'radicalize': 1,\n",
       " 'hogan': 36,\n",
       " 'genderisms': 1,\n",
       " 'compliance': 5,\n",
       " 'finster': 2,\n",
       " 'giulia': 3,\n",
       " 'tween': 6,\n",
       " 'insides': 7,\n",
       " 'depicted': 209,\n",
       " 'weds': 1,\n",
       " 'cavil': 1,\n",
       " 'minutest': 1,\n",
       " 'lale': 2,\n",
       " 'perceptions': 12,\n",
       " 'intensities': 1,\n",
       " 'bairns': 1,\n",
       " 'seftel': 8,\n",
       " 'rodeos': 1,\n",
       " 'grisham': 14,\n",
       " 'abdullah': 1,\n",
       " 'village': 242,\n",
       " 'parnell': 7,\n",
       " 'grassroots': 1,\n",
       " 'metschurat': 4,\n",
       " 'faceful': 1,\n",
       " 'notion': 101,\n",
       " 'enrico': 16,\n",
       " 'misfilmed': 1,\n",
       " 'mineo': 1,\n",
       " 'rumbustious': 1,\n",
       " 'filler': 68,\n",
       " 'hahk': 11,\n",
       " 'crossover': 10,\n",
       " 'kabuto': 3,\n",
       " 'unmatchable': 1,\n",
       " 'gourmet': 1,\n",
       " 'jurisprudence': 1,\n",
       " 'someones': 24,\n",
       " 'cyclist': 2,\n",
       " 'adopted': 75,\n",
       " 'vehicles': 58,\n",
       " 'cushing': 69,\n",
       " 'willett': 3,\n",
       " 'downscale': 1,\n",
       " 'vic': 34,\n",
       " 'gamekeeper': 1,\n",
       " 'link': 97,\n",
       " 'laments': 7,\n",
       " 'wilson': 197,\n",
       " 'ussr': 15,\n",
       " 'megabomb': 1,\n",
       " 'palates': 2,\n",
       " 'insectish': 1,\n",
       " 'sexualities': 1,\n",
       " 'total': 628,\n",
       " 'particulars': 4,\n",
       " 'electronica': 3,\n",
       " 'cruthers': 1,\n",
       " 'woodsball': 2,\n",
       " 'feinnnes': 1,\n",
       " 'brokers': 1,\n",
       " 'swallow': 58,\n",
       " 'officials': 38,\n",
       " 'insomniac': 25,\n",
       " 'crackles': 4,\n",
       " 'caudillos': 1,\n",
       " 'illuminators': 1,\n",
       " 'kindergarteners': 1,\n",
       " 'thanku': 1,\n",
       " 'hooky': 2,\n",
       " 'britons': 5,\n",
       " 'carridine': 3,\n",
       " 'procrastination': 1,\n",
       " 'famarialy': 1,\n",
       " 'darndest': 1,\n",
       " 'overemotes': 1,\n",
       " 'weighting': 2,\n",
       " 'intensity': 154,\n",
       " 'predict': 69,\n",
       " 'minimally': 8,\n",
       " 'mentioned': 542,\n",
       " 'runs': 484,\n",
       " 'prayed': 15,\n",
       " 'satta': 2,\n",
       " 'ambiance': 41,\n",
       " 'mutation': 5,\n",
       " 'rochester': 151,\n",
       " 'breeding': 18,\n",
       " 'logothetis': 1,\n",
       " 'bjore': 1,\n",
       " 'okerlund': 1,\n",
       " 'sometime': 62,\n",
       " 'cambpell': 3,\n",
       " 'decapitates': 4,\n",
       " 'pinkie': 2,\n",
       " 'blackberry': 2,\n",
       " 'skywriting': 1,\n",
       " 'envisaged': 3,\n",
       " 'byington': 6,\n",
       " 'eréndira': 1,\n",
       " 'briss': 2,\n",
       " 'claymore': 4,\n",
       " 'tito': 28,\n",
       " 'transformations': 22,\n",
       " 'imaginaire': 1,\n",
       " 'wagons': 2,\n",
       " 'reflecting': 25,\n",
       " 'repudiate': 1,\n",
       " 'urinate': 5,\n",
       " 'chamionship': 1,\n",
       " 'egotism': 3,\n",
       " 'ching': 21,\n",
       " 'arrives': 160,\n",
       " 'nrk': 1,\n",
       " 'showbiz': 11,\n",
       " 'ktla': 2,\n",
       " 'overturn': 1,\n",
       " 'shrubbery': 2,\n",
       " 'kimbo': 1,\n",
       " 'exclusively': 49,\n",
       " 'crowley': 7,\n",
       " 'carribien': 1,\n",
       " 'misery': 87,\n",
       " 'hicock': 1,\n",
       " 'filthiest': 1,\n",
       " 'latched': 2,\n",
       " 'censorship': 38,\n",
       " 'smelt': 2,\n",
       " 'mantagna': 1,\n",
       " 'gestapo': 10,\n",
       " 'tricked': 37,\n",
       " 'viennese': 11,\n",
       " 'dangers': 35,\n",
       " 'jerusalem': 14,\n",
       " 'tremblay': 2,\n",
       " 'dogie': 1,\n",
       " 'katchuck': 1,\n",
       " 'whodunit': 24,\n",
       " 'mercies': 5,\n",
       " 'quickly': 623,\n",
       " 'sentimentality': 57,\n",
       " 'oratory': 4,\n",
       " 'duchess': 25,\n",
       " 'hamil': 3,\n",
       " 'bosnian': 11,\n",
       " 'fanatics': 33,\n",
       " 'crusierweight': 1,\n",
       " 'farenheight': 1,\n",
       " 'compilation': 26,\n",
       " 'lordy': 2,\n",
       " 'psychoanalyze': 1,\n",
       " 'prolong': 5,\n",
       " 'imprecating': 1,\n",
       " 'believers': 19,\n",
       " 'backwards': 65,\n",
       " 'kappor': 1,\n",
       " 'ferrer': 22,\n",
       " 'gaunts': 1,\n",
       " 'augustin': 1,\n",
       " 'rethought': 1,\n",
       " 'seven': 324,\n",
       " 'gisborne': 1,\n",
       " 'seeding': 1,\n",
       " 'obituaries': 1,\n",
       " 'improvisation': 12,\n",
       " 'becoming': 347,\n",
       " 'book': 2320,\n",
       " 'venessa': 2,\n",
       " 'den': 22,\n",
       " 'satana': 1,\n",
       " 'og': 1,\n",
       " 'nelkin': 4,\n",
       " 'briton': 1,\n",
       " 'intimist': 1,\n",
       " 'zaara': 1,\n",
       " 'maiden': 33,\n",
       " 'whoop': 4,\n",
       " 'launching': 22,\n",
       " 'jars': 11,\n",
       " 'mon': 14,\n",
       " 'darkplace': 1,\n",
       " 'completion': 10,\n",
       " 'drip': 11,\n",
       " 'innit': 1,\n",
       " 'design': 328,\n",
       " 'residuals': 1,\n",
       " 'environment': 163,\n",
       " 'jacketed': 2,\n",
       " 'chaperone': 2,\n",
       " 'perennial': 21,\n",
       " 'unobservant': 3,\n",
       " 'santas': 3,\n",
       " 'davids': 1,\n",
       " 'defeatism': 1,\n",
       " 'eavesdrops': 1,\n",
       " 'laker': 1,\n",
       " 'dale': 30,\n",
       " 'shapiro': 18,\n",
       " 'originate': 5,\n",
       " 'criminology': 1,\n",
       " 'caribou': 7,\n",
       " 'robbi': 1,\n",
       " 'kascier': 1,\n",
       " 'inventively': 4,\n",
       " 'strasberg': 3,\n",
       " 'paramount': 74,\n",
       " 'volé': 1,\n",
       " 'shiktak': 1,\n",
       " 'climates': 3,\n",
       " 'stimulates': 6,\n",
       " 'gwyenth': 1,\n",
       " 'sumptuous': 22,\n",
       " 'mill': 70,\n",
       " 'kaczmarek': 1,\n",
       " 'bocho': 1,\n",
       " 'frog': 51,\n",
       " 'ravensbrück': 2,\n",
       " 'muslims': 48,\n",
       " 'chaser': 2,\n",
       " 'preponderance': 4,\n",
       " 'raider': 24,\n",
       " 'nakano': 1,\n",
       " 'leisin': 4,\n",
       " 'mercurio': 5,\n",
       " 'falling': 378,\n",
       " 'fabio': 8,\n",
       " 'cinderella': 229,\n",
       " 'kazakh': 5,\n",
       " 'confidentially': 1,\n",
       " 'manucci': 1,\n",
       " 'notebook': 15,\n",
       " 'egocentric': 6,\n",
       " 'tristram': 3,\n",
       " 'pseuds': 1,\n",
       " 'apke': 1,\n",
       " 'pengiun': 1,\n",
       " 'fife': 4,\n",
       " 'vainly': 7,\n",
       " 'patrician': 3,\n",
       " 'joists': 1,\n",
       " 'eugenio': 1,\n",
       " 'rip': 165,\n",
       " 'sets': 847,\n",
       " 'practiced': 14,\n",
       " 'tolerant': 17,\n",
       " 'montage': 102,\n",
       " 'gener': 1,\n",
       " 'mccord': 8,\n",
       " 'em': 30,\n",
       " 'pepi': 7,\n",
       " 'soma': 1,\n",
       " 'etzel': 7,\n",
       " 'volunteer': 21,\n",
       " 'befell': 3,\n",
       " 'menalaus': 1,\n",
       " 'readily': 53,\n",
       " 'loughlin': 2,\n",
       " 'tremont': 1,\n",
       " 'arrivé': 1,\n",
       " 'mccenna': 1,\n",
       " 'shameless': 45,\n",
       " 'griffths': 2,\n",
       " 'prosthesis': 1,\n",
       " 'ser': 1,\n",
       " 'lizards': 11,\n",
       " 'chide': 1,\n",
       " 'wedded': 2,\n",
       " 'videotaping': 4,\n",
       " 'keira': 27,\n",
       " 'vials': 3,\n",
       " 'robinson': 92,\n",
       " 'massachusett': 1,\n",
       " 'jarred': 4,\n",
       " 'tsu': 1,\n",
       " 'luca': 5,\n",
       " 'citizens': 73,\n",
       " 'nicmart': 1,\n",
       " 'goksal': 2,\n",
       " 'torpedoes': 1,\n",
       " 'sandro': 5,\n",
       " 'riget': 12,\n",
       " 'rodriquez': 1,\n",
       " 'regulated': 3,\n",
       " 'amoung': 1,\n",
       " 'ishwar': 14,\n",
       " 'launchers': 7,\n",
       " 'acker': 1,\n",
       " 'doctors': 66,\n",
       " 'henley': 3,\n",
       " 'mithun': 1,\n",
       " 'direfully': 1,\n",
       " 'lagravenese': 1,\n",
       " 'ethnically': 1,\n",
       " 'accomplice': 22,\n",
       " 'brazilian': 41,\n",
       " 'kaafi': 1,\n",
       " 'ashely': 1,\n",
       " 'crafty': 15,\n",
       " 'terrorist': 96,\n",
       " 'leight': 1,\n",
       " 'guildernstern': 1,\n",
       " 'backsides': 2,\n",
       " 'woodmobile': 1,\n",
       " 'crashed': 40,\n",
       " 'yeoman': 6,\n",
       " 'locks': 32,\n",
       " 'garcíadiego': 1,\n",
       " 'scenarist': 4,\n",
       " 'dipaolo': 1,\n",
       " 'valued': 11,\n",
       " 'dythirambic': 1,\n",
       " 'decommissioned': 1,\n",
       " 'spine': 48,\n",
       " 'electrifying': 21,\n",
       " 'unevenly': 5,\n",
       " 'heroically': 4,\n",
       " 'straights': 4,\n",
       " 'provide': 303,\n",
       " 'manifested': 13,\n",
       " 'swede': 5,\n",
       " 'româniei': 2,\n",
       " 'hunger': 28,\n",
       " 'undertitles': 1,\n",
       " 'tkachenko': 1,\n",
       " 'reinterpret': 1,\n",
       " 'reservedly': 1,\n",
       " 'notations': 1,\n",
       " 'fillet': 1,\n",
       " 'moulds': 2,\n",
       " 'menopuasal': 1,\n",
       " 'haff': 3,\n",
       " 'translating': 11,\n",
       " 'jugars': 1,\n",
       " 'skating': 19,\n",
       " 'caveat': 5,\n",
       " 'scallops': 1,\n",
       " 'knitted': 4,\n",
       " 'industrialize': 1,\n",
       " 'padmé': 1,\n",
       " 'ix': 4,\n",
       " 'antiseptic': 3,\n",
       " 'jugnu': 1,\n",
       " 'acquiescence': 2,\n",
       " 'acoustics': 2,\n",
       " 'surrendering': 4,\n",
       " 'immortal': 58,\n",
       " 'dexterous': 2,\n",
       " 'buntao': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sorted_vocab_freq = list(reversed(sorted(vocab_freq.items(), key=operator.itemgetter(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71238"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_vocab_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOTAL_VOCAB = 5000\n",
    "\n",
    "word_to_id = dict()\n",
    "id_to_word = dict()\n",
    "for i in range(TOTAL_VOCAB):\n",
    "    word_to_id[sorted_vocab_freq[i][0]] = i\n",
    "    id_to_word[i] = sorted_vocab_freq[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'br'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Len\n",
       "0       89\n",
       "1      226\n",
       "2       56\n",
       "3       66\n",
       "4       76\n",
       "5       25\n",
       "6       29\n",
       "7      237\n",
       "8      166\n",
       "9      227\n",
       "10      52\n",
       "11     275\n",
       "12      56\n",
       "13      92\n",
       "14     119\n",
       "15     180\n",
       "16     107\n",
       "17     171\n",
       "18     260\n",
       "19      52\n",
       "20     154\n",
       "21     134\n",
       "22      53\n",
       "23      54\n",
       "24     338\n",
       "25     105\n",
       "26      18\n",
       "27     132\n",
       "28      49\n",
       "29      97\n",
       "...    ...\n",
       "24970  113\n",
       "24971   74\n",
       "24972   75\n",
       "24973  189\n",
       "24974   59\n",
       "24975  153\n",
       "24976   79\n",
       "24977   88\n",
       "24978  154\n",
       "24979  102\n",
       "24980   29\n",
       "24981  176\n",
       "24982  118\n",
       "24983   53\n",
       "24984   68\n",
       "24985  499\n",
       "24986  145\n",
       "24987  142\n",
       "24988   44\n",
       "24989   66\n",
       "24990   55\n",
       "24991   99\n",
       "24992   46\n",
       "24993  115\n",
       "24994   53\n",
       "24995   86\n",
       "24996   74\n",
       "24997   59\n",
       "24998  165\n",
       "24999  116\n",
       "\n",
       "[25000 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_lengths = pd.DataFrame([len(review) for review in reviews])\n",
    "review_lengths.columns = ['Len']\n",
    "\n",
    "review_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>118.36848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>89.42677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>63.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>88.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>144.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1409.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Len\n",
       "count  25000.00000\n",
       "mean     118.36848\n",
       "std       89.42677\n",
       "min        4.00000\n",
       "25%       63.00000\n",
       "50%       88.00000\n",
       "75%      144.00000\n",
       "max     1409.00000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265.5, -58.5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removal of outliers using Tukey's Method\n",
    "first_q = review_lengths.Len.quantile([0.25])[0.25]\n",
    "third_q = review_lengths.Len.quantile([0.75])[0.75]\n",
    "\n",
    "upper_threshold = third_q + 1.5*(third_q-first_q)\n",
    "lower_threshold = first_q - 1.5*(third_q-first_q)\n",
    "\n",
    "upper_threshold,lower_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(l):\n",
    "    new_l = []\n",
    "    for word in l:\n",
    "        if word in word_to_id:\n",
    "            new_l.append(word_to_id[word])\n",
    "    return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    converted_review = convert(reviews[i])\n",
    "    if len(converted_review) <= upper_threshold:\n",
    "        X_train.append(converted_review)\n",
    "        y_train.append(data_train['Sentiment'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=int(upper_threshold),value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24011, 265), (24011,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.concat([pos_test_data,neg_test_data],ignore_index = True)\n",
    "data_test = data_test.sample(frac=0.3).reset_index(drop=True)\n",
    "\n",
    "validation_reviews = []\n",
    "\n",
    "for index,row in data_test.iterrows():\n",
    "    text = (row['Text'].lower())\n",
    "    validation_reviews.append(textclean(text))\n",
    "    \n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    converted_review = convert(validation_reviews[i])\n",
    "    if len(converted_review) <= upper_threshold:\n",
    "        X_val.append(converted_review)\n",
    "        y_val.append(data_test['Sentiment'][i])\n",
    "        \n",
    "X_val = np.array(X_val)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=int(upper_threshold),value = 0)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7238, 265)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 265, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 265, 128)          12416     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 265, 64)           24640     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 265, 32)           4128      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 265, 16)           1040      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4240)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4240)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               424100    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 626,425\n",
      "Trainable params: 626,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout,Conv1D,Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "EMBEDDING_LEN = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(TOTAL_VOCAB,EMBEDDING_LEN,input_length = int(upper_threshold)))\n",
    "model.add(Conv1D(128,3,padding = 'same'))\n",
    "model.add(Conv1D(64,3,padding = 'same'))\n",
    "model.add(Conv1D(32,2,padding = 'same'))\n",
    "model.add(Conv1D(16,2,padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24011 samples, validate on 7238 samples\n",
      "Epoch 1/3\n",
      "24011/24011 [==============================] - 85s 4ms/step - loss: 0.3936 - acc: 0.8102 - val_loss: 0.3251 - val_acc: 0.8625\n",
      "Epoch 2/3\n",
      "24011/24011 [==============================] - 80s 3ms/step - loss: 0.2586 - acc: 0.8940 - val_loss: 0.3198 - val_acc: 0.8647\n",
      "Epoch 3/3\n",
      "24011/24011 [==============================] - 80s 3ms/step - loss: 0.2099 - acc: 0.9181 - val_loss: 0.3673 - val_acc: 0.8541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff56798cf98>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,validation_data = (X_val,y_val),epochs = 3,batch_size = 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
